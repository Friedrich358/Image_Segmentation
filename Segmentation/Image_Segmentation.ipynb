{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506467f",
   "metadata": {},
   "source": [
    "# Image segmentation\n",
    "\n",
    "The task is to train a simple deep learning model to segment the cells visible in wide fluorescence images.\n",
    "For this I use the <b>'tensorflow.keras'</b> library because it is quite easy to use and provides all the necessary tools.\n",
    "\n",
    "I downloaded the images and put them in folders right next to the Python program. \\\n",
    "The folders are called <b>'img_seg'</b> (the masks) and <b>'img_full'</b> (the normal images). \n",
    "\n",
    "```\n",
    "/ \n",
    "+--img_seg \n",
    "+--img_full \n",
    "+--Image_Segmentation.ipynb\n",
    "```\n",
    "\n",
    "Each dataset consisted of wide-field epifluorescence images of cultured neurons with both cytoplasmic (phalloidin) and nuclear staining (DAPI) and a set of manual segmentations of the neuronal and nuclear boundaries.\n",
    "\n",
    "As far as I can tell, the sets of manual segmentations, the masks, are all from the images with the cytoplasmic (phalloidin) channel. \n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"https://cildata.crbs.ucsd.edu/display_images/ccdb/ccdb_512/6843_512r.jpg\" width=\"250px\"></td>\n",
    "<td><img src=\"https://cildata.crbs.ucsd.edu/display_images/ccdb/ccdb_512/6843_512s.jpg\" width=\"250px\"></td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "therefore I decided to only use them to train my model.\n",
    "\n",
    "Since only some of the wide-field epifluorescence images have more than one corresponding mask, i also decided to use only one mask for each image. This still leaves me with 100 images and 100 masks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78badbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tf_k                              #This library is for modeling the network.\n",
    "\n",
    "\n",
    "import os                                                    # This two librarys are used to load\n",
    "import matplotlib.image as img                               # the data from the folders into the Program\n",
    "\n",
    "\n",
    "import numpy as np                                           # To reshape and to normalize the data,\n",
    "from skimage.transform import downscale_local_mean as dlm    # i use this two librarys.\n",
    "\n",
    "import matplotlib.pyplot as plt                              # And this library is for ploting the results\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b90ee",
   "metadata": {},
   "source": [
    "The function Looks through both folders and loads only the images that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ead5baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_images(foldername_1='img_seg', foldername_2='img_full'):\n",
    "    masks = []\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(foldername_1):\n",
    "        if \"GT_01\" in filename:\n",
    "            masks.append(  img.imread(  os.path.join(foldername_1, filename))[:,:,0]   )\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    for filename in os.listdir(foldername_2):\n",
    "        if \"w1\" in filename:\n",
    "            images.append( dlm( img.imread(  os.path.join(foldername_2, filename)  ), (2,2) )  )\n",
    "        else:\n",
    "            pass\n",
    "    return masks, images\n",
    "\n",
    "\n",
    "load_y, load_x = load_training_images()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0599f",
   "metadata": {},
   "source": [
    "Normalize data by dividing all values by the highest value. \\\n",
    "I use the highest value of all the images, not just the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3c66be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Normalize(Liste):\n",
    "    max_value = np.max(Liste)\n",
    "    for k in range(len(Liste)):\n",
    "        Liste[k] = Liste[k]/max_value\n",
    "    return Liste\n",
    "\n",
    "norm_y_train = Normalize(load_y)\n",
    "norm_x_train = Normalize(load_x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57246370",
   "metadata": {},
   "source": [
    "Divides the data in training and in testing data, and brings them in the right shape \\\n",
    "The percentage attribute is the ratio of training and test data. By default the ratio is 70 to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab966087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(masks, images, percentage=70):\n",
    "    \n",
    "    masks_train = np.array(masks).reshape(100,520,696,1)[:percentage]\n",
    "    images_train = np.array(images).reshape(100,520,696,1)[:percentage]\n",
    "    masks_test = np.array(masks).reshape(100,520,696,1)[percentage:100]\n",
    "    images_test  = np.array(images).reshape(100,520,696,1)[percentage:100]\n",
    "    \n",
    "    return masks_train, images_train, masks_test, images_test\n",
    "\n",
    "\n",
    "y_train, x_train, y_test, x_test = get_data(norm_y_train, norm_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905fe78",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "\n",
    "The architecture im uesing is a normal <b> U-Net architecture </b>, which i made a bit smaller than it is usually. \\\n",
    "I did this becaus becaus i got performanc problems while running the program. So i ended up uesing only two <b> channe sizes, 8 and 16 </b>. \n",
    "\n",
    "The shape of the data goes from <b>520x696x1</b> to <b>260x348x8</b> to <b>130x174x16</b> and then back again.\n",
    "\n",
    "I also use a <b>dropout of 30%</b> to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d57e50c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The U-Net:\n",
    "\n",
    "\n",
    "def get_model(chanels=[8, 16], drop_rate=0.3):\n",
    "\n",
    "    inputs = tf_k.Input(shape=(520,696,1))                                       \n",
    "\n",
    "    c1 = tf_k.layers.Conv2D(chanels[0], 3, activation='relu', padding=\"same\")(inputs)\n",
    "    c1 = tf_k.layers.Dropout(drop_rate)(c1)\n",
    "    c1 = tf_k.layers.Conv2D(chanels[0], 3, activation='relu', padding=\"same\")(c1)\n",
    "\n",
    "    p1 = tf_k.layers.MaxPool2D((2,2))(c1)                                        \n",
    "\n",
    "    c2 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(p1)\n",
    "    c2 = tf_k.layers.Dropout(drop_rate)(c2)\n",
    "    c2 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(c2)  \n",
    "\n",
    "    p2 = tf_k.layers.MaxPool2D((2,2))(c2)                                        \n",
    "\n",
    "    c3 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(p2)\n",
    "    c3 = tf_k.layers.Dropout(drop_rate)(c3)\n",
    "    c3 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(c3)\n",
    "\n",
    "    u1 = tf_k.layers.Conv2DTranspose(16, 2, strides=2, padding=\"same\")(c3)\n",
    "    u1 = tf_k.layers.concatenate([c2,u1])\n",
    "\n",
    "    c4 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(u1)\n",
    "    c4 = tf_k.layers.Dropout(drop_rate)(c4)\n",
    "    c4 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(c4)\n",
    "\n",
    "    u2 = tf_k.layers.Conv2DTranspose(chanels[0], 2, strides=2, padding=\"same\")(c4)\n",
    "    u2 = tf_k.layers.concatenate([c1,u2])\n",
    "\n",
    "    c5 = tf_k.layers.Conv2D(chanels[0], 3, activation='relu', padding=\"same\")(u2)\n",
    "    c5 = tf_k.layers.Dropout(drop_rate)(c5)\n",
    "    c5 = tf_k.layers.Conv2D(chanels[0], 3, activation='relu', padding=\"same\")(c5)\n",
    "\n",
    "\n",
    "    outputs = tf_k.layers.Conv2D(1, 1, activation='sigmoid', padding=\"same\")(c5) \n",
    "    \n",
    "    return tf_k.Model(inputs=inputs, outputs=outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f31abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "14/14 [==============================] - 46s 3s/step - loss: 0.6765 - accuracy: 0.7420\n",
      "Epoch 2/3\n",
      "14/14 [==============================] - 45s 3s/step - loss: 0.5656 - accuracy: 0.7705\n",
      "Epoch 3/3\n",
      " 3/14 [=====>........................] - ETA: 35s - loss: 0.3755 - accuracy: 0.8738"
     ]
    }
   ],
   "source": [
    "my_model = get_model()\n",
    "my_model.compile(optimizer=\"Adam\", loss=\"BinaryCrossentropy\", metrics=[\"accuracy\"])\n",
    "my_model.fit(x_train, y_train, epochs=3, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.evaluate(x_test, y_test, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de656eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 8\n",
    "predictions = my_model(x_test[index:index+1]).numpy()\n",
    "predictions2 = my_model(x_train[index:index+1]).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(20,10))\n",
    "ax[0][0].imshow(x_train[index])\n",
    "ax[0][1].imshow(predictions2[0])\n",
    "ax[0][2].imshow(y_train[index])\n",
    "\n",
    "\n",
    "ax[1][0].imshow(x_test[index])\n",
    "ax[1][1].imshow(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a21711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
