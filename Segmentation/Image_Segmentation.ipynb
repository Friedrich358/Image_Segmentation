{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f506467f",
   "metadata": {},
   "source": [
    "# Image segmentation\n",
    "\n",
    "The task is to train a simple deep learning model to segment the cells visible in wide fluorescence images. \\\n",
    "I decided too use a commenly U-Net architecture for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630019b",
   "metadata": {},
   "source": [
    "I have use the folowing librarys.\n",
    "\n",
    "<b> tensorflow.keras </b> for implementing the network, <b> os </b> and <b> matplotlib.image </b> to load the images, <b> numpy </b> and <b> skimage.transform </b> to reshape the images and last but not least <b> matplotlib.pyplot </b> to plot my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478cb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tf_k                              \n",
    "import os                                                     \n",
    "import matplotlib.image as img                               \n",
    "import numpy as np                                           \n",
    "from skimage.transform import downscale_local_mean as dlm    \n",
    "import matplotlib.pyplot as plt                              \n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523aa15e",
   "metadata": {},
   "source": [
    "## Task 1: Download the data\n",
    " \n",
    "I downloaded the images and put them in folders right next to the Python program. \\\n",
    "The folders are called <b>'Masks'</b> and <b>'Images'</b>. \n",
    "\n",
    "```\n",
    "/ \n",
    "+--Masks \n",
    "+--Images \n",
    "+--Image_Segmentation.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a7476",
   "metadata": {},
   "source": [
    "## Task 2: Analyzing and pre-processing\n",
    "\n",
    "Each dataset consisted of wide-field epifluorescence images of cultured neurons with both cytoplasmic (phalloidin) and nuclear staining (DAPI) and a set of manual segmentations of the neuronal and nuclear boundaries.\n",
    "```\n",
    "One Set:\n",
    "    1x image with cytoplasmic (phalloidin) staining    size: 1040x1392x1\n",
    "    1x image with nuclear staining                     size: 1040x1392x1\n",
    "    up too 3 masks                                     size:  520x 696x3\n",
    "```\n",
    "\n",
    "\n",
    "As far as I can tell, all the masks are segmented from the images with the cytoplasmic (phalloidin) channel. \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://cildata.crbs.ucsd.edu/display_images/ccdb/ccdb_512/6843_512r.jpg\" width=\"300px\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://cildata.crbs.ucsd.edu/display_images/ccdb/ccdb_512/6843_512s.jpg\" width=\"300px\">\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td><center>\n",
    "            Wide field fluorescent image of cultured <br />\n",
    "            neuroblastoma cells labeled with phalloidn <br /> \n",
    "            FITC (green) and DAPI (blue) nuclear stain.\n",
    "        </center></td>\n",
    "        <td><center>\n",
    "            Manual segmentation of cell and nuclear <br />\n",
    "            boundaries of double labeled cultured neurons.\n",
    "        </center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Therefore I decided to only use them to train my model.\n",
    "\n",
    "Furthermore only a few of the images have 3 corresponding mask and some have only one. Becaus of that i only use one mask for image. \\\n",
    "That leaves me with 100 images and 100 masks.\n",
    "\n",
    "The next step is to downsample the images and reshape the masks, so that they both have the same size of 520x696x1. \\\n",
    "This two task are done by the firs function. The function Looks through both folders and loads only the images that are needed.\n",
    "Then at the same time it brings the images and the masks into the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ead5baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_images(foldername_1='Masks', foldername_2='Images'):\n",
    "    masks = []\n",
    "    images = []\n",
    "    \n",
    "    for filename in os.listdir(foldername_1):\n",
    "        if \"GT_01\" in filename:\n",
    "            masks.append(  img.imread(  os.path.join(foldername_1, filename))[:,:,0]   )\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    for filename in os.listdir(foldername_2):\n",
    "        if \"w1\" in filename:\n",
    "            images.append( dlm( img.imread(  os.path.join(foldername_2, filename)  ), (2,2) )  )\n",
    "        else:\n",
    "            pass\n",
    "    return masks, images\n",
    "\n",
    "\n",
    "load_y, load_x = load_training_images()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd04827",
   "metadata": {},
   "source": [
    "The next function, normalizes the data by dividing all values by the highest value. \\\n",
    "I use the highest value of all the images, not just the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3c66be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Normalize(Liste):\n",
    "    max_value = np.max(Liste)\n",
    "    for k in range(len(Liste)):\n",
    "        Liste[k] = Liste[k]/max_value\n",
    "    return Liste\n",
    "\n",
    "norm_y_train = Normalize(load_y)\n",
    "norm_x_train = Normalize(load_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca8298",
   "metadata": {},
   "source": [
    "The last step is to divide the data in training and in testing data, and one last time to bring them in the right shape. \\\n",
    "I use 70 images for training and 30 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab966087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(masks, images, size_train_data=70):\n",
    "    \n",
    "    masks_train = np.array(masks).reshape(100,520,696,1)[:size_train_data]\n",
    "    images_train = np.array(images).reshape(100,520,696,1)[:size_train_data]\n",
    "    masks_test = np.array(masks).reshape(100,520,696,1)[size_train_data:100]\n",
    "    images_test  = np.array(images).reshape(100,520,696,1)[size_train_data:100]\n",
    "    \n",
    "    return masks_train, images_train, masks_test, images_test\n",
    "\n",
    "\n",
    "y_train, x_train, y_test, x_test = get_data(norm_y_train, norm_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905fe78",
   "metadata": {},
   "source": [
    "## Task 3: The model\n",
    "\n",
    "The architecture im uesing is a normal <b> U-Net architecture </b>, which i made a bit smaller than it is usually. \\\n",
    "I did this becaus becaus i got performanc problems while running the program. So i ended up uesing only two <b> channe sizes, 8 and 16 </b>. \n",
    "\n",
    "The shape of the data goes from <b>520x696x1</b> to <b>260x348x8</b> to <b>130x174x16</b> and then back again.\n",
    " \n",
    "For the Optemizer i chose the <b> Adam </b> algorithm and my loss-function is the <b> BinaryCrossentropy </b>, as I only have to classes which I want to differentiate. \\\n",
    "I also use a <b>dropout of 30%</b> to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d57e50c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model(chanels=[8, 16], drop_rate=0.3):\n",
    "\n",
    "    inputs = tf_k.Input(shape=(520,696,1))                                       \n",
    "\n",
    "    c1 = tf_k.layers.Conv2D(chanels[0], 3, activation='relu', padding=\"same\")(inputs)\n",
    "    c1 = tf_k.layers.Dropout(drop_rate)(c1)\n",
    "    c1 = tf_k.layers.Conv2D(chanels[0], 3, activation='relu', padding=\"same\")(c1)\n",
    "\n",
    "    p1 = tf_k.layers.MaxPool2D((2,2))(c1)                                        \n",
    "\n",
    "    c2 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(p1)\n",
    "    c2 = tf_k.layers.Dropout(drop_rate)(c2)\n",
    "    c2 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(c2)  \n",
    "\n",
    "    p2 = tf_k.layers.MaxPool2D((2,2))(c2)                                        \n",
    "\n",
    "    c3 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(p2)\n",
    "    c3 = tf_k.layers.Dropout(drop_rate)(c3)\n",
    "    c3 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(c3)\n",
    "\n",
    "    u1 = tf_k.layers.Conv2DTranspose(16, 2, strides=2, padding=\"same\")(c3)\n",
    "    u1 = tf_k.layers.concatenate([c2,u1])\n",
    "\n",
    "    c4 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(u1)\n",
    "    c4 = tf_k.layers.Dropout(drop_rate)(c4)\n",
    "    c4 = tf_k.layers.Conv2D(chanels[1], 3, activation='relu', padding=\"same\")(c4)\n",
    "\n",
    "    u2 = tf_k.layers.Conv2DTranspose(chanels[0], 2, strides=2, padding=\"same\")(c4)\n",
    "    u2 = tf_k.layers.concatenate([c1,u2])\n",
    "\n",
    "    c5 = tf_k.layers.Conv2D(chanels[0], 3, activation='relu', padding=\"same\")(u2)\n",
    "    c5 = tf_k.layers.Dropout(drop_rate)(c5)\n",
    "    c5 = tf_k.layers.Conv2D(chanels[0], 3, activation='relu', padding=\"same\")(c5)\n",
    "\n",
    "\n",
    "    outputs = tf_k.layers.Conv2D(1, 1, activation='sigmoid', padding=\"same\")(c5) \n",
    "    \n",
    "    return tf_k.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40b46a",
   "metadata": {},
   "source": [
    "Fro the trainig i use 70 sets of images and masks, in batches of 5 at a time with 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ae9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 3/14 [=====>........................] - ETA: 35s - loss: 0.6918 - accuracy: 0.5765"
     ]
    }
   ],
   "source": [
    "my_model = get_model()\n",
    "my_model.compile(optimizer=\"Adam\", loss=\"BinaryCrossentropy\", metrics=[\"accuracy\"])\n",
    "my_model.fit(x_train, y_train, epochs=3, batch_size=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937edec8",
   "metadata": {},
   "source": [
    "Fro the testing i batches of 5, but this time only with 30 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.evaluate(x_test, y_test, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de656eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 8\n",
    "predictions = my_model(x_test[index:index+1]).numpy()\n",
    "predictions2 = my_model(x_train[index:index+1]).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(20,10))\n",
    "ax[0][0].imshow(x_train[index])\n",
    "ax[0][1].imshow(predictions2[0])\n",
    "ax[0][2].imshow(y_train[index])\n",
    "\n",
    "\n",
    "ax[1][0].imshow(x_test[index])\n",
    "ax[1][1].imshow(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a21711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
